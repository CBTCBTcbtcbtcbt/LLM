# LLM Client Library

ä¸€ä¸ªç®€æ´é€šç”¨çš„LLMå®¢æˆ·ç«¯åº“ï¼Œæ”¯æŒæ‰€æœ‰OpenAIå…¼å®¹çš„APIæ¥å£ã€‚

## ç‰¹æ€§

- ğŸš€ åŸºäºOpenAIå®˜æ–¹SDKï¼Œç¨³å®šå¯é 
- ğŸ”Œ æ”¯æŒæ‰€æœ‰OpenAIå…¼å®¹çš„APIï¼ˆOpenAIã€Azureã€å›½å†…å„å¤§æ¨¡å‹ç­‰ï¼‰
- ğŸ’¬ æ”¯æŒæµå¼å’Œéæµå¼å¯¹è¯
- ğŸ¤– å†…ç½®Agentç³»ç»Ÿï¼Œå¯åˆ›å»ºå…·æœ‰ç‰¹å®šè§’è‰²çš„AIåŠ©æ‰‹
- ğŸ“ è‡ªåŠ¨ç®¡ç†å¯¹è¯å†å²
- âš™ï¸ çµæ´»çš„é…ç½®ç³»ç»Ÿ

## å®‰è£…

```bash
pip install -r requirements.txt
```

## å¿«é€Ÿå¼€å§‹

### 1. é…ç½®API

ç¼–è¾‘ `config.yaml` æ–‡ä»¶ï¼š

```yaml
api:
  api_key: "your-api-key"
  base_url: "https://api.openai.com/v1"  # æˆ–å…¶ä»–å…¼å®¹çš„APIåœ°å€
  model: "gpt-3.5-turbo"
  temperature: 0.7
  max_tokens: 2000
```

### 2. åŸºç¡€å¯¹è¯

```python
from llm_client import LLMClient
from conversation import Conversation

# åˆ›å»ºå®¢æˆ·ç«¯
client = LLMClient(
    api_key="your-api-key",
    base_url="https://api.openai.com/v1",
    model="gpt-3.5-turbo"
)

# åˆ›å»ºå¯¹è¯
conv = Conversation(client)

# å‘é€æ¶ˆæ¯
response = conv.send("Hello!")
print(response)
```

### 3. æµå¼å¯¹è¯

```python
# æµå¼è¾“å‡º
for chunk in conv.stream_send("Tell me a story"):
    print(chunk, end="", flush=True)
```

### 4. åˆ›å»ºAI Agent

```python
from agent import Agent

# åˆ›å»ºå…·æœ‰ç‰¹å®šè§’è‰²çš„Agent
teacher = Agent(
    client=client,
    name="Teacher",
    role="You are a patient teacher.",
    personality="Friendly and encouraging"
)

response = teacher.respond("Explain quantum physics")
print(response)
```

## æ ¸å¿ƒç»„ä»¶

### LLMClient

é€šç”¨LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒæ‰€æœ‰OpenAIå…¼å®¹çš„APIã€‚

**å‚æ•°ï¼š**
- `api_key`: APIå¯†é’¥
- `base_url`: APIåŸºç¡€URL
- `model`: æ¨¡å‹åç§°
- `temperature`: æ¸©åº¦å‚æ•°ï¼ˆ0-1ï¼‰
- `max_tokens`: æœ€å¤§tokenæ•°
- `**kwargs`: å…¶ä»–é¢å¤–å‚æ•°

**æ–¹æ³•ï¼š**
- `chat(messages, **kwargs)`: å‘é€å¯¹è¯è¯·æ±‚ï¼Œè¿”å›å®Œæ•´å“åº”
- `stream_chat(messages, **kwargs)`: æµå¼å‘é€å¯¹è¯è¯·æ±‚

### Conversation

å¯¹è¯ç®¡ç†å™¨ï¼Œè‡ªåŠ¨ç»´æŠ¤å¯¹è¯å†å²ã€‚

**å‚æ•°ï¼š**
- `client`: LLMClientå®ä¾‹
- `system_prompt`: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰

**æ–¹æ³•ï¼š**
- `send(message, **kwargs)`: å‘é€æ¶ˆæ¯å¹¶è·å–å“åº”
- `stream_send(message, **kwargs)`: æµå¼å‘é€æ¶ˆæ¯
- `clear()`: æ¸…ç©ºå¯¹è¯å†å²
- `get_history()`: è·å–å¯¹è¯å†å²
- `set_system_prompt(prompt)`: è®¾ç½®ç³»ç»Ÿæç¤ºè¯

### Agent

AIä»£ç†ï¼Œå…·æœ‰ç‰¹å®šè§’è‰²å’Œä¸ªæ€§ã€‚

**å‚æ•°ï¼š**
- `client`: LLMClientå®ä¾‹
- `name`: Agentåç§°
- `role`: è§’è‰²æè¿°
- `personality`: ä¸ªæ€§æè¿°

**æ–¹æ³•ï¼š**
- `respond(message, **kwargs)`: ç”Ÿæˆå“åº”
- `stream_respond(message, **kwargs)`: æµå¼ç”Ÿæˆå“åº”
- `reset()`: é‡ç½®å¯¹è¯å†å²
- `get_history()`: è·å–å¯¹è¯å†å²

## ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šç®€å•å¯¹è¯

```python
from llm_client import LLMClient
from conversation import Conversation

client = LLMClient(
    api_key="sk-xxx",
    base_url="https://api.openai.com/v1",
    model="gpt-3.5-turbo"
)

conv = Conversation(client, system_prompt="You are a helpful assistant.")
response = conv.send("What's the weather like?")
print(response)
```

### ç¤ºä¾‹2ï¼šå¤šè½®å¯¹è¯

```python
conv = Conversation(client)

conv.send("My name is Alice")
conv.send("What's my name?")  # AIä¼šè®°ä½ä½ çš„åå­—
```

### ç¤ºä¾‹3ï¼šåˆ›å»ºä¸“ä¸šAgent

```python
from agent import Agent

# åˆ›å»ºä»£ç åŠ©æ‰‹
coder = Agent(
    client=client,
    name="Coder",
    role="You are an expert programmer.",
    personality="Concise and technical"
)

code = coder.respond("Write a Python function to calculate fibonacci")
print(code)
```

### ç¤ºä¾‹4ï¼šä½¿ç”¨ä¸åŒçš„APIæä¾›å•†

```python
# ä½¿ç”¨Azure OpenAI
client = LLMClient(
    api_key="your-azure-key",
    base_url="https://your-resource.openai.azure.com/openai/deployments/your-deployment",
    model="gpt-35-turbo"
)

# ä½¿ç”¨å›½å†…APIï¼ˆå¦‚æ™ºè°±ã€é€šä¹‰ç­‰ï¼‰
client = LLMClient(
    api_key="your-key",
    base_url="https://api.provider.com/v1",
    model="model-name"
)
```

### ç¤ºä¾‹5ï¼šè‡ªå®šä¹‰å‚æ•°

```python
# åˆ›å»ºæ—¶è®¾ç½®é»˜è®¤å‚æ•°
client = LLMClient(
    api_key="sk-xxx",
    base_url="https://api.openai.com/v1",
    model="gpt-4",
    temperature=0.9,
    max_tokens=4000
)

# è°ƒç”¨æ—¶è¦†ç›–å‚æ•°
response = conv.send("Tell me a story", temperature=1.0, max_tokens=1000)
```

## è¿è¡Œç¤ºä¾‹

```bash
# è¿è¡Œäº¤äº’å¼èŠå¤©
python chat.py

# è¿è¡Œç®€å•å¯¹è¯ç¤ºä¾‹
python examples/simple_chat.py

# è¿è¡ŒAgentç¤ºä¾‹
python examples/agent_example.py
```

## å…¼å®¹çš„APIæä¾›å•†

æœ¬åº“æ”¯æŒæ‰€æœ‰OpenAIå…¼å®¹çš„APIï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š

- OpenAI
- Azure OpenAI
- æ™ºè°±AI (GLM)
- é€šä¹‰åƒé—® (Qwen)
- æ–‡å¿ƒä¸€è¨€ (ERNIE)
- è®¯é£æ˜Ÿç« (Spark)
- Moonshot AI
- DeepSeek
- å…¶ä»–æä¾›OpenAIå…¼å®¹æ¥å£çš„æœåŠ¡

## é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯

```python
conv = Conversation(
    client,
    system_prompt="You are a professional translator. Translate everything to Chinese."
)
```

### ç®¡ç†å¯¹è¯å†å²

```python
# è·å–å†å²
history = conv.get_history()

# æ¸…ç©ºå†å²
conv.clear()

# ä¿®æ”¹ç³»ç»Ÿæç¤ºè¯
conv.set_system_prompt("New system prompt")
```

### æµå¼è¾“å‡ºæ§åˆ¶

```python
full_response = ""
for chunk in conv.stream_send("Write a poem"):
    full_response += chunk
    print(chunk, end="", flush=True)
print(f"\n\nFull response length: {len(full_response)}")
```

## é¡¹ç›®ç»“æ„

```
LLM/
â”œâ”€â”€ llm_client.py      # æ ¸å¿ƒLLMå®¢æˆ·ç«¯
â”œâ”€â”€ conversation.py    # å¯¹è¯ç®¡ç†
â”œâ”€â”€ agent.py          # Agentç³»ç»Ÿ
â”œâ”€â”€ chat.py           # äº¤äº’å¼èŠå¤©ç•Œé¢
â”œâ”€â”€ config.yaml       # é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements.txt  # ä¾èµ–
â”œâ”€â”€ README.md         # ä½¿ç”¨æ‰‹å†Œ
â””â”€â”€ examples/         # ç¤ºä¾‹ä»£ç 
    â”œâ”€â”€ simple_chat.py
    â””â”€â”€ agent_example.py
```

## æ³¨æ„äº‹é¡¹

1. è¯·å¦¥å–„ä¿ç®¡APIå¯†é’¥ï¼Œä¸è¦æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ
2. ä¸åŒçš„APIæä¾›å•†å¯èƒ½æœ‰ä¸åŒçš„æ¨¡å‹åç§°å’Œå‚æ•°è¦æ±‚
3. æ³¨æ„APIè°ƒç”¨çš„è´¹ç”¨å’Œé€Ÿç‡é™åˆ¶
4. æµå¼è¾“å‡ºæ—¶éœ€è¦æ­£ç¡®å¤„ç†å¼‚å¸¸

## è®¸å¯è¯

MIT License

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼
